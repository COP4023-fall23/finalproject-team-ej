{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Sentiment Analysis project<h1>\n",
    "<h3>Name student: Ennis McCorvey IV<h3>\n",
 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },

#Data Wrangling

file_path = 'customer_reviewers.tsv'
reviewers_df = pd.read_csv('customer_reviewers.tsv',sep='\t')
print(reviewers_df.head(10))


# Check for missing values in the entire DataFrame
print(" Checking for missing values ")
print(reviewers_df.isnull().sum())
miss_rows = reviewers_df[reviewers_df.isnull().any(axis=1)]
print(miss_rows)

#Removing Stopwords
def remove_stopwords(review):
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(review)
    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]
    return ' '.join(filtered_text)

reviewers_df['Cleaned_Review'] = reviewers_df['Checked_Review'].apply(remove_stopwords)

# Display the DataFrame with non-stop words
print(reviewers_df)

# Concatenate all cleaned reviews into a single string
all_reviews = ' '.join(reviewers_df['Cleaned_Review'])

# Generate WordCloud
wordcloud = WordCloud(width=800, height=400, max_font_size = 100, background_color='yellow').generate(text)

# Display the WordCloud 
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

#Data Engineering

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform the target labels
encoded_labels = label_encoder.fit_transform()

# Print the original labels and their corresponding encoded values
for original_label, encoded_label in zip(targeencoded_labels):
    print(f"{original_label}: {encoded_label}")

 # Create a Tokenizer
tokenize = Tokenizer()

# Fit the tokenizer on the reviews
tokenize.fit_on_texts(reviewers_df['Cleaned_Review'])

# Convert the reviews to sequences
sequences = tokenizer.texts_to_sequences(reviewers_df['Cleaned_Review'])

# Print the tokenized sequences
print("Tokenized Sequences:")
print(sequences)

#Model Design

# Convert labels to one-hot encoded format
labels_one_hot = to_categorical(labels, num_classes=2)

# Define the model
model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=120, input_length=padded_sequences.shape[1]))
model.add(SpatialDropout1D(0.4))
model.add(LSTM(units=176, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(units=2, activation='softmax'))

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Display the model summary
print(model.summary())


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    padded_sequences, 
    labels_one_hot, 
    test_size=0.2, 
    random_state=42
)

X_train = padded_sequences(X_train)
X_test = padded_sequences(X_test)
y_train = padded_sequences(y_train)
y_test = padded_sequences(y_test)


# Train the model
model.fit( X_train, y_train, epochs=5, batch_size=32, verbose='auto', validation_data=(X_test, y_test)) 

#Evaluate the model
predictions = model.predict(X_test)
predicted_labels = np.argmax(predictions, axis=1)
true_labels = np.argmax(y_test, axis=1)
    
#Perform Evaluation
accuracy = accuracy_score(true_labels, predicted_labels)
print("Accuracy is:", accuracy)

print("\nClassification Report:")
print(classification_report(true_labels, predicted_labels))

report = confusion_matrix(true_labels, predicted_labels)
print("\nCorresponding Metric Evaluation:")
print(report)

  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
